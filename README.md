# Graduate-Design

In this paper, we study the problem of Conversational Question Generation (CQG). Different from the standalone interaction in general Question Generation (QG), CQG is grounded in a passage and combines conversation history to generate interconnected questions in a question-answering style conversation. Conventional sequence-to-sequence neural models have tackled massive tasks for QG, while in CQG, they are liable to lose key information from passage or conversation history, and even tend to produce repetitive words, affecting the answerability of generated questions. To address these issues, we propose a novel multi-source pointer network with coverage mechanism. Our multi-source pointer network has two basic mechanisms: the attention-mixed mechanism jointly calculates the attention distribution of passage and conversation history, which predicts the output probability distribution of words; the copy-generation mechanism decides when to copy a word from the passage or the conversation history and when to generate a word from the predefined vocabulary, which aids accurate copy of information form source text and retains the ability of generating new words. Our coverage mechanism keeps track of what words have been generated, which resolves the repetition in generated questions. Empirical results on CoQA dataset demonstrate that our model outperforms the other stateof-the-art baselines. Moreover, our system effectively reduces the word duplication rates and significantly minimizes the number of generated questions with the repetition.
